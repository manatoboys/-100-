{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tajiri/.local/lib/python3.8/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/tajiri/.local/lib/python3.8/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/tajiri/.local/lib/python3.8/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch.nn.functional as F\n",
    "\n",
    "FILE_PATH = \"./wiki_corpus_2.01/kyoto_lexicon.csv\"\n",
    "tokenizer_src = get_tokenizer('spacy', language='ja_core_news_sm')\n",
    "tokenizer_tgt = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "# どれか一つの行がうまく読み込めなかったため、truncate_tagged_linesでfield数が一致しない行は無視している(元データ51983行)\n",
    "\n",
    "class datasets(Dataset):\n",
    "    def __init__(self, text, label):\n",
    "        self.jp_datas = text\n",
    "        self.en_datas = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len (self.jp_datas)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        jp = self.jp_datas[index]\n",
    "        en = self.en_datas[index]\n",
    "        return jp,en\n",
    "\n",
    "class DataLoaderCreater:\n",
    "\n",
    "    def __init__(self, file_path, src_tokenizer, tgt_tokenizer):\n",
    "        self.file_path = file_path\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def build_vocab(self, texts, tokenizer):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            counter.update(tokenizer(text))\n",
    "        specials = ['<unk>', '<pad>', '<start>', '<end>']\n",
    "        v = vocab(counter, specials=specials, min_freq=1)\n",
    "        v.set_default_index(v['<unk>'])\n",
    "        return v\n",
    "\n",
    "    def convert_text_to_indexes(self, text, vocab, tokenizer):\n",
    "        return [vocab['<start>']] + [\n",
    "            vocab[token] if token in vocab else vocab['<unk>'] for token in tokenizer(text.strip(\"\\n\"))\n",
    "        ] + [vocab['<end>']]\n",
    "\n",
    "    def create_dataloader(self):\n",
    "        df = pl.read_csv(self.file_path, separator=\",\", encoding=\"utf-8\", has_header=True, truncate_ragged_lines=True)\n",
    "        df_selected = df.select([df.columns[0], df.columns[1]])\n",
    "        df_jp = df_selected[:, 0]\n",
    "        jp_list = df_jp.to_list()\n",
    "        df_en = df_selected[:, 1]\n",
    "        en_list = df_en.to_list()\n",
    "\n",
    "        self.vocab_src = self.build_vocab(jp_list, tokenizer_src)\n",
    "        self.vocab_tgt = self.build_vocab(en_list, tokenizer_tgt)\n",
    "        self.vocab_src_index_to_word = self.vocab_src.get_itos()\n",
    "        self.vocab_tgt_index_to_word = self.vocab_tgt.get_itos()\n",
    "        self.vocab_src = self.vocab_src.get_stoi()\n",
    "        self.vocab_tgt = self.vocab_tgt.get_stoi()\n",
    "        self.len_src_vocab = len(self.vocab_src)\n",
    "        self.len_tgt_vocab = len(self.vocab_tgt)\n",
    "\n",
    "        src_data = pad_sequence([torch.tensor(self.convert_text_to_indexes(text, self.vocab_src, tokenizer=self.src_tokenizer)) for text in jp_list], batch_first = True, padding_value = self.vocab_src[\"<pad>\"])\n",
    "        tgt_data = pad_sequence([torch.tensor(self.convert_text_to_indexes(text, self.vocab_tgt, tokenizer=self.tgt_tokenizer)) for text in en_list], batch_first = True, padding_value = self.vocab_tgt[\"<pad>\"])\n",
    "\n",
    "        dataset = datasets(src_data, tgt_data)\n",
    "\n",
    "        # データセットの長さを取得\n",
    "        dataset_length = len(dataset)\n",
    "\n",
    "        # 各分割のサイズを計算\n",
    "        train_size = int(0.8 * dataset_length)\n",
    "        val_size = int(0.1 * dataset_length)\n",
    "        test_size = dataset_length - train_size - val_size\n",
    "\n",
    "        # データセットをランダムに分割\n",
    "        train_dataset, valid_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=128)\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size=128)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=128)\n",
    "\n",
    "        return train_dataloader, test_dataloader, valid_dataloader\n",
    "\n",
    "dataloader_creater = DataLoaderCreater(FILE_PATH, tokenizer_src, tokenizer_tgt)\n",
    "train_dataloader, test_dataloader, valid_dataloader = dataloader_creater.create_dataloader()\n",
    "src_vocab_size = dataloader_creater.len_src_vocab\n",
    "tgt_vocab_size = dataloader_creater.len_tgt_vocab\n",
    "vocab_src = dataloader_creater.vocab_src\n",
    "vocab_tgt = dataloader_creater.vocab_tgt\n",
    "# インデックス列を文字列に変換\n",
    "vacab_src_index_to_word = dataloader_creater.vocab_src_index_to_word\n",
    "vacab_tgt_index_to_word = dataloader_creater.vocab_tgt_index_to_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "# EncodingとEmbeddingの違い\n",
    "# 学習を行わないのがPositional Encoding\n",
    "# 学習を行うのがPositional Embedding\n",
    "\n",
    "class PositionalEncoding():\n",
    "    def __init__(self, embedding_dim, len_sequence):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.len_sequence = len_sequence\n",
    "\n",
    "    def get_sin(self,i,k):\n",
    "        return math.sin(i/(10000)**(k/self.len_sequence))\n",
    "\n",
    "    def get_cos(self,i,k):\n",
    "        return math.cos(i/(10000)**(k/self.len_sequence))\n",
    "    \n",
    "    def get_positional_vector(self):\n",
    "        pe = torch.zeros(self.len_sequence, self.embedding_dim)\n",
    "        for pos in range(self.len_sequence):\n",
    "            for i in range(0, int(self.embedding_dim/2)):\n",
    "                pe[pos, 2*i] = self.get_sin(pos, i)\n",
    "                pe[pos, 2*i+1] = self.get_cos(pos, i)\n",
    "        return pe\n",
    "\n",
    "\n",
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size_src, vocab_size_tgt, embedding_dim, num_heads, num_layers, device,  dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # Positional Encoderを加算する必要あり\n",
    "        self.embedding_src = nn.Embedding(vocab_size_src, embedding_dim)\n",
    "        self.embedding_tgt = nn.Embedding(vocab_size_tgt, embedding_dim)\n",
    "        self.pos_embedding_src = PositionalEncoding(embedding_dim, 33)\n",
    "        self.pos_embedding_tgt = PositionalEncoding(embedding_dim, 73)\n",
    "        self.transformer = nn.Transformer(d_model=embedding_dim, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size_tgt)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding_src(src)\n",
    "        tgt = self.embedding_tgt(tgt)\n",
    "        batch_size = src.shape[0]\n",
    "        pos_src = self.pos_embedding_src.get_positional_vector().to(device)\n",
    "        pos_src = pos_src.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        pos_tgt = self.pos_embedding_tgt.get_positional_vector().to(device)\n",
    "        pos_tgt = pos_tgt.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        src = src + pos_src\n",
    "        tgt = tgt + pos_tgt\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc_out(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "# モデルのハイパーパラメータ\n",
    "embedding_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "device = torch.device('cuda:7' if torch.cuda.is_available() else 'cpu')\n",
    "# モデルの初期化\n",
    "# モデルの初期化\n",
    "model = TransformerModel(src_vocab_size, tgt_vocab_size, embedding_dim, num_heads, num_layers, device, dropout=0).to(device)\n",
    "\n",
    "# 保存された重みをロード\n",
    "model.load_state_dict(torch.load(\"./model_weight.pth\", map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 慈恩 大師 基\n",
      "output: Realms Realms Realms Realms Realms Realms Realms Realms Realms Realms Realms Realms Realms TAKITA Realms Realms Realms county Seito Realms Realms Realms county county county county Asura county TAKITA county Asura county TAKITA county county county county county Hirotsune Hirotsune Hirotsune county Hirotsune county They They They Hirotsune Hirotsune They Hirotsune They They county They They They county They Hirotsune county Hirotsune Hirotsune county Hirotsune Hirotsune Hirotsune Hirotsune Hirotsune Hirotsune Hirotsune Hirotsune\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# インデックスから単語に変換する関数\n",
    "def indexes_to_sentence(indexes, vocab_index_to_word):\n",
    "    return ' '.join([vocab_index_to_word[idx] for idx in indexes if vocab_index_to_word[idx]not in [\"<pad>\", \"<start>\", \"<end>\"]])\n",
    "\n",
    "# 翻訳関数\n",
    "def translate_sentence(model, sentence, src_vocab, tgt_vocab, src_tokenizer, src_vocab_index_to_word, tgt_vocab_index_to_word, device):\n",
    "    model.eval()\n",
    "    tokens = [src_vocab['<start>']] + [src_vocab.get(token, src_vocab['<unk>']) for token in src_tokenizer(sentence)] + [src_vocab['<end>']]\n",
    "    pad_len = 33 - len(tokens)\n",
    "    for i in range(pad_len):\n",
    "        tokens.append(src_vocab[\"<pad>\"])\n",
    "    src_tensor = torch.tensor(tokens).unsqueeze(0).to(device)\n",
    "    tgt = [tgt_vocab[\"<start>\"]]\n",
    "    len_pad = 73 - len(tgt)\n",
    "    for i in range(len_pad):\n",
    "        tgt.append(tgt_vocab[\"<pad>\"])\n",
    "    tgt_tensor = torch.tensor(tgt).to(device).unsqueeze(0)\n",
    "    for i in range(73 - 1):\n",
    "        output = model(src_tensor, tgt_tensor)\n",
    "        output = output.argmax(dim = 2)\n",
    "        tgt = output[0,:i+2].tolist()\n",
    "        tgt_pad_len = 73 - len(tgt)\n",
    "        for k in range(tgt_pad_len):\n",
    "            tgt.append(tgt_vocab[\"<pad>\"])\n",
    "        tgt_tensor = torch.tensor(tgt).unsqueeze(0).to(device)\n",
    "    \n",
    "    translated_sentence = indexes_to_sentence(output[0], tgt_vocab_index_to_word)\n",
    "    return translated_sentence\n",
    "\n",
    "# 任意の日本語の文章\n",
    "jp_sentence = test_dataloader.dataset\n",
    "jp_sentence = indexes_to_sentence(jp_sentence[0][0], vacab_src_index_to_word)\n",
    "print(f\"input: {jp_sentence}\")\n",
    "\n",
    "# 翻訳の実行\n",
    "translated_sentence = translate_sentence(model, jp_sentence, vocab_src, vocab_tgt, tokenizer_src, vacab_src_index_to_word, vacab_tgt_index_to_word, device)\n",
    "print(f\"output: {translated_sentence}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
