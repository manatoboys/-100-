{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tajiri/.local/lib/python3.8/site-packages/torchtext/data/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/tajiri/.local/lib/python3.8/site-packages/torchtext/vocab/__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "/home/tajiri/.local/lib/python3.8/site-packages/torchtext/utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import torch\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import vocab\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "\n",
    "JP_TRAIN_FILE_PATH = \"./kftt-data-1.0/data/orig/kyoto-train.ja\"\n",
    "EN_TRAIN_FILE_PATH = \"./kftt-data-1.0/data/orig/kyoto-train.en\"\n",
    "\n",
    "tokenizer_src = get_tokenizer('spacy', language='ja_core_news_sm')\n",
    "tokenizer_tgt = get_tokenizer('spacy', language='en_core_web_sm')\n",
    "\n",
    "with open(JP_TRAIN_FILE_PATH, \"r\", encoding=\"utf-8\")as f:\n",
    "    train_jp_list = f.readlines()\n",
    "    train_jp_list = [jp.strip(\"\\n\") for jp in train_jp_list]\n",
    "\n",
    "with open(EN_TRAIN_FILE_PATH, \"r\", encoding=\"utf-8\")as f:\n",
    "    train_en_list = f.readlines()\n",
    "    train_en_list = [en.strip(\"\\n\") for en in train_en_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 62\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m src_batch, tgt_batch\n\u001b[1;32m     61\u001b[0m dataloader_creater \u001b[38;5;241m=\u001b[39m DataLoaderCreater(tokenizer_src, tokenizer_tgt)\n\u001b[0;32m---> 62\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43mdataloader_creater\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjp_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_jp_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43men_list\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_en_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m vocab_size_src \u001b[38;5;241m=\u001b[39m dataloader_creater\u001b[38;5;241m.\u001b[39mvocab_size_src\n\u001b[1;32m     64\u001b[0m vocab_size_tgt \u001b[38;5;241m=\u001b[39m dataloader_creater\u001b[38;5;241m.\u001b[39mvocab_size_tgt\n",
      "Cell \u001b[0;32mIn[33], line 35\u001b[0m, in \u001b[0;36mDataLoaderCreater.create_dataloader\u001b[0;34m(self, jp_list, en_list, collate_fn)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_dataloader\u001b[39m(\u001b[38;5;28mself\u001b[39m, jp_list, en_list, collate_fn):\n\u001b[0;32m---> 35\u001b[0m     vocab_src \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild_vocab\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjp_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer_src\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     36\u001b[0m     vocab_tgt \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuild_vocab(en_list, tokenizer_tgt)\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_src_itos \u001b[38;5;241m=\u001b[39m vocab_src\u001b[38;5;241m.\u001b[39mget_itos()\n",
      "Cell \u001b[0;32mIn[33], line 23\u001b[0m, in \u001b[0;36mDataLoaderCreater.build_vocab\u001b[0;34m(self, texts, tokenizer)\u001b[0m\n\u001b[1;32m     21\u001b[0m counter \u001b[38;5;241m=\u001b[39m Counter()\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m---> 23\u001b[0m     counter\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     24\u001b[0m specials \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<unk>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<pad>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<start>\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m<end>\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     25\u001b[0m v \u001b[38;5;241m=\u001b[39m vocab(counter, specials\u001b[38;5;241m=\u001b[39mspecials, min_freq\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchtext/data/utils.py:14\u001b[0m, in \u001b[0;36m_spacy_tokenize\u001b[0;34m(x, spacy)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_spacy_tokenize\u001b[39m(x, spacy):\n\u001b[0;32m---> 14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [tok\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m \u001b[43mspacy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/spacy/lang/ja/__init__.py:56\u001b[0m, in \u001b[0;36mJapaneseTokenizer.__call__\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Doc:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# convert sudachipy.morpheme.Morpheme to DetailedToken and merge continuous spaces\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     sudachipy_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m     dtokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_dtokens(sudachipy_tokens)\n\u001b[1;32m     58\u001b[0m     dtokens, spaces \u001b[38;5;241m=\u001b[39m get_dtokens_and_spaces(dtokens, text)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class datasets(Dataset):\n",
    "    def __init__(self, text, label):\n",
    "        self.jp_datas = text\n",
    "        self.en_datas = label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.jp_datas)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        jp = self.jp_datas[index]\n",
    "        en = self.en_datas[index]\n",
    "        return jp,en\n",
    "\n",
    "class DataLoaderCreater:\n",
    "\n",
    "    def __init__(self, src_tokenizer, tgt_tokenizer):\n",
    "        self.src_tokenizer = src_tokenizer\n",
    "        self.tgt_tokenizer = tgt_tokenizer\n",
    "\n",
    "    def build_vocab(self, texts, tokenizer):\n",
    "        counter = Counter()\n",
    "        for text in texts:\n",
    "            counter.update(tokenizer(text))\n",
    "        specials = ['<unk>', '<pad>', '<start>', '<end>']\n",
    "        v = vocab(counter, specials=specials, min_freq=1)\n",
    "        v.set_default_index(v['<unk>'])\n",
    "        return v\n",
    "\n",
    "    def convert_text_to_indexes(self, text, vocab, tokenizer):\n",
    "        return [vocab['<start>']] + [\n",
    "            vocab[token] if token in vocab else vocab['<unk>'] for token in tokenizer(text.strip(\"\\n\"))\n",
    "        ] + [vocab['<end>']]\n",
    "\n",
    "    def create_dataloader(self, jp_list, en_list, collate_fn):\n",
    "        vocab_src = self.build_vocab(jp_list, tokenizer_src)\n",
    "        vocab_tgt = self.build_vocab(en_list, tokenizer_tgt)\n",
    "        self.vocab_src_itos = vocab_src.get_itos()\n",
    "        self.vocab_tgt_itos = vocab_tgt.get_itos()\n",
    "        self.vocab_src_stoi = vocab_src.get_stoi()\n",
    "        self.vocab_tgt_stoi = vocab_tgt.get_stoi()\n",
    "        self.vocab_size_src = len(self.vocab_src_stoi)\n",
    "        self.vocab_size_tgt = len(self.vocab_tgt_stoi)\n",
    "\n",
    "        src_data = [torch.tensor(self.convert_text_to_indexes(jp_data, self.vocab_src_stoi, self.src_tokenizer)) for jp_data in jp_list]\n",
    "        tgt_data = [torch.tensor(self.convert_text_to_indexes(en_data, self.vocab_tgt_stoi, self.tgt_tokenizer)) for en_data in en_list]\n",
    "        dataset = datasets(src_data, tgt_data)\n",
    "\n",
    "        dataloader = DataLoader(dataset, batch_size=64, collate_fn=collate_fn, shuffle=True)\n",
    "\n",
    "        return dataloader\n",
    "\n",
    "PADDING_ID = 1\n",
    "\n",
    "def collate_fn(batch):\n",
    "    src_batch, tgt_batch = zip(*batch)\n",
    "\n",
    "    src_batch = pad_sequence(src_batch, padding_value=PADDING_ID,  batch_first=True)\n",
    "    tgt_batch = pad_sequence(tgt_batch, padding_value=PADDING_ID, batch_first=True)\n",
    "    return src_batch, tgt_batch\n",
    "\n",
    "dataloader_creater = DataLoaderCreater(tokenizer_src, tokenizer_tgt)\n",
    "train_dataloader = dataloader_creater.create_dataloader(jp_list=train_jp_list, en_list=train_en_list, collate_fn=collate_fn)\n",
    "vocab_size_src = dataloader_creater.vocab_size_src\n",
    "vocab_size_tgt = dataloader_creater.vocab_size_tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "# EncodingとEmbeddingの違い\n",
    "# 学習を行わないのがPositional Encoding\n",
    "# 学習を行うのがPositional Embedding\n",
    "\n",
    "class PositionalEncoding():\n",
    "    def __init__(self, embedding_dim, len_sequence):\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.len_sequence = len_sequence\n",
    "\n",
    "    def get_sin(self,i,k):\n",
    "        return torch.sin(torch.tensor(i/(10000)**(k/self.len_sequence)))\n",
    "\n",
    "    def get_cos(self,i,k):\n",
    "        return torch.cos(torch.tensor(i/(10000)**(k/self.len_sequence)))\n",
    "\n",
    "    def get_positional_vector(self):\n",
    "        pe = torch.zeros(self.len_sequence, self.embedding_dim)\n",
    "        for pos in range(self.len_sequence):\n",
    "            for i in range(0, int(self.embedding_dim/2)):\n",
    "                pe[pos, 2*i] = self.get_sin(pos, i)\n",
    "                pe[pos, 2*i+1] = self.get_cos(pos, i)\n",
    "        return pe\n",
    "\n",
    "\n",
    "# Transformerモデルの定義\n",
    "class TransformerModel(nn.Module):\n",
    "    def __init__(self, vocab_size_src, vocab_size_tgt, embedding_dim, num_heads, num_layers, device,  dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.device = device\n",
    "        # Positional Encoderを加算する必要あり\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_src = nn.Embedding(vocab_size_src, embedding_dim)\n",
    "        self.embedding_tgt = nn.Embedding(vocab_size_tgt, embedding_dim)\n",
    "        self.transformer = nn.Transformer(d_model=embedding_dim, nhead=num_heads, num_encoder_layers=num_layers, num_decoder_layers=num_layers, dropout=dropout, batch_first=True)\n",
    "        self.fc_out = nn.Linear(embedding_dim, vocab_size_tgt)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        src = self.embedding_src(src)\n",
    "        tgt = self.embedding_tgt(tgt)\n",
    "        batch_size = src.shape[0]\n",
    "        pos_src = PositionalEncoding(self.embedding_dim, src.shape[1]).get_positional_vector().to(self.device)\n",
    "        pos_src = pos_src.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        pos_tgt = PositionalEncoding(self.embedding_dim, tgt.shape[1]).get_positional_vector().to(self.device)\n",
    "        pos_tgt = pos_tgt.unsqueeze(0).repeat(batch_size, 1, 1)\n",
    "        src = src + pos_src\n",
    "        tgt = tgt + pos_tgt\n",
    "        output = self.transformer(src, tgt)\n",
    "        output = self.fc_out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 88, 179946])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.89 GiB. GPU \u0006 has a total capacity of 47.54 GiB of which 1.25 GiB is free. Process 2514640 has 29.87 GiB memory in use. Process 2757014 has 1.32 GiB memory in use. Including non-PyTorch memory, this process has 15.09 GiB memory in use. Of the allocated memory 9.85 GiB is allocated by PyTorch, and 4.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss\u001b[38;5;241m/\u001b[39m\u001b[38;5;28mlen\u001b[39m(dataloader)\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(num_epochs)):\n\u001b[0;32m---> 34\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (epoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m%\u001b[39m\u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Train Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[32], line 28\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, criterion, optimizer)\u001b[0m\n\u001b[1;32m     26\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m) \u001b[38;5;66;03m#なぜ(バッチサイズ、　シークエンス長、　vocab_size) => (バッチサイズ、　vocab_size、　シークエンス長)にする必要があるのか\u001b[39;00m\n\u001b[1;32m     27\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, tgt)\n\u001b[0;32m---> 28\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m##ここが問題　r\u001b[39;00m\n\u001b[1;32m     29\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     30\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:525\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    517\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    518\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    523\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    524\u001b[0m     )\n\u001b[0;32m--> 525\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    526\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    527\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:267\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    262\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 267\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/graph.py:744\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    742\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    743\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    745\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    746\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    747\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    748\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.89 GiB. GPU \u0006 has a total capacity of 47.54 GiB of which 1.25 GiB is free. Process 2514640 has 29.87 GiB memory in use. Process 2757014 has 1.32 GiB memory in use. Including non-PyTorch memory, this process has 15.09 GiB memory in use. Of the allocated memory 9.85 GiB is allocated by PyTorch, and 4.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# モデルのハイパーパラメータ\n",
    "embedding_dim = 512\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "lr_rate = 1e-5\n",
    "\n",
    "device = torch.device('cuda:6' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# モデルの初期化\n",
    "model = TransformerModel(vocab_size_src, vocab_size_tgt, embedding_dim, num_heads, num_layers, device).to(device)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PADDING_ID)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr_rate)\n",
    "\n",
    "# トレーニングループ\n",
    "num_epochs = 100\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        src = src.to(device)\n",
    "        tgt = tgt.to(device)\n",
    "        output = model(src, tgt)\n",
    "        print(output.shape)\n",
    "        output = output.permute(0, 2, 1) #なぜ(バッチサイズ、　シークエンス長、　vocab_size) => (バッチサイズ、　vocab_size、　シークエンス長)にする必要があるのか\n",
    "        loss = criterion(output, tgt)\n",
    "        loss.backward()##ここが問題　r\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss/len(dataloader)\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    train_loss = train_epoch(model, train_dataloader, criterion, optimizer)\n",
    "    if (epoch+1)%10 == 0:\n",
    "        print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(),'model_weight.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
