{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "\n",
    "# 単語ベクトルモデルの読み込み\n",
    "model_path = './GoogleNews-vectors-negative300.bin.gz'  # モデルのパス\n",
    "model = KeyedVectors.load_word2vec_format(model_path, binary=True)  # binary=Trueはバイナリ形式のモデルの場合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 行を削除(余計な文章が入っているため)\n",
    "FILE_PATH = \"./questions-words.txt\"\n",
    "NEWFILE_PATH = \"./questions-words-without-first-row.txt\"\n",
    "\n",
    "with open(FILE_PATH, \"r\", encoding=\"utf-8\")as f1, open(NEWFILE_PATH, \"w\", encoding=\"utf-8\")as f2:\n",
    "    data_list = f1.readlines()\n",
    "\n",
    "    for data in data_list:\n",
    "        if data[0]==\":\":\n",
    "            category = data.replace(\" \",\"\").replace(\":\",\"\").strip(\"\\n\")\n",
    "        else:\n",
    "            datas = data.split()\n",
    "            datas.append(category)\n",
    "            data_str = \"\\t\".join(datas)\n",
    "            print(data_str, file = f2)\n",
    "\n",
    "\n",
    "\n",
    "    # data = [data for data in data_list if data[0] != \":\"] #余計な行を削除\n",
    "    # f2.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(NEWFILE_PATH, names=[\"vec1\", \"vec2\", \"vec3\", \"vec4\", \"category\", \"top_vec\", \"cos_similarity\"], sep=\"\\t\")\n",
    "# 列ごとに抽出\n",
    "vec1_series = df.loc[:,\"vec1\"]\n",
    "vec2_series = df.loc[:,\"vec2\"]\n",
    "vec3_series = df.loc[:,\"vec3\"]\n",
    "\n",
    "df['top_vec'] = df['top_vec'].astype(str)\n",
    "for i, (vec1, vec2, vec3) in enumerate(zip(vec1_series, vec2_series, vec3_series)):\n",
    "    try:\n",
    "        similar_vec = model.most_similar(positive = [vec2, vec3], negative = [vec1], topn = 1)\n",
    "        df.loc[i,\"top_vec\"] = similar_vec[0][0]\n",
    "        df.loc[i,\"cos_similarity\"] = similar_vec[0][1]\n",
    "    except Exception as e:\n",
    "        print(f\"Error ad index{i}: {e}\")\n",
    "\n",
    "df.to_csv(\"output-questions-words.txt\", sep=\"\\t\", index=None, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==**出力例**==  \n",
    "Athens\tGreece\tBaghdad\tIraq\tcapital-common-countries\tIraqi\t0.6351870894432068  \n",
    "Athens\tGreece\tBangkok\tThailand\tcapital-common-countries\tThailand\t0.7137669324874878  \n",
    "Athens\tGreece\tBeijing\tChina\tcapital-common-countries\tChina\t0.7235777378082275  \n",
    "Athens\tGreece\tBerlin\tGermany\tcapital-common-countries\tGermany\t0.6734622716903687  \n",
    "Athens\tGreece\tBern\tSwitzerland\tcapital-common-countries\tSwitzerland\t0.4919748306274414  \n",
    "Athens\tGreece\tCairo\tEgypt\tcapital-common-countries\tEgypt\t0.7527809739112854  \n",
    "Athens\tGreece\tCanberra\tAustralia\tcapital-common-countries\tAustralia\t0.5837326645851135  \n",
    "Athens\tGreece\tHanoi\tVietnam\tcapital-common-countries\tViet_Nam\t0.6276342272758484  \n",
    "Athens\tGreece\tHavana\tCuba\tcapital-common-countries\tCuba\t0.6460990905761719  \n",
    "Athens\tGreece\tHelsinki\tFinland\tcapital-common-countries\tFinland\t0.6899982690811157  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
