{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Morph():\n",
    "    def __init__(self, doc_list):\n",
    "        self.surface = doc_list[0]\n",
    "        self.base = doc_list[7]\n",
    "        self.pos = doc_list[1]\n",
    "        self.pos1= doc_list[2]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{\" + f\"surface: {self.surface}, base: {self.base}, pos: {self.pos}, pos1: {self.pos1}\" + \"}\"\n",
    "\n",
    "class Chunk():\n",
    "    def __init__(self,morph_list,dst,srcs):\n",
    "        self.morph_list = morph_list\n",
    "        self.dst = int(dst)\n",
    "        self.srcs = srcs\n",
    "\n",
    "    def __str__(self):\n",
    "        morphs_str = ', '.join(str(morph) for morph in self.morph_list) # Morphオブジェクトのリストを文字列に変換\n",
    "        return f\"Morphs: [{morphs_str}], Dst: {self.dst}, Srcs: {self.srcs}\"\n",
    "\n",
    "# 文節番号と係り先番号の取得\n",
    "def find_feature_number(text):\n",
    "    pattern = r\"^\\* \\d+ ([^\\d]*\\d+)D\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        dst_number = match.group(1)  # 文節番号  # 係先番号\n",
    "        return dst_number\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"ai.ja.txt.parsed\", mode=\"r\", encoding=\"utf-8\")as f:\n",
    "    docs = f.readlines()\n",
    "\n",
    "Chunk_list = []\n",
    "chunk_flag = False\n",
    "# srcの追加処理は後で行う\n",
    "for doc in docs[5:]:\n",
    "    if doc == \"EOS\\n\":\n",
    "        chunk = Chunk(morph_list,dst_num,[])\n",
    "        Chunk_list.append(chunk)\n",
    "        break\n",
    "    # *から始まる行とEOSの行は飛ばす\n",
    "    if doc[0]==\"*\":\n",
    "        if chunk_flag:\n",
    "            chunk = Chunk(morph_list,dst_num,[])\n",
    "            Chunk_list.append(chunk)\n",
    "        dst_num = find_feature_number(doc)\n",
    "        morph_list = []\n",
    "        chunk_flag = True\n",
    "    else:\n",
    "        doc = doc.replace(\"\\t\",\",\").replace(\"\\n\", \"\")\n",
    "        doc_list = doc.split(\",\")\n",
    "        doc_morph = Morph(doc_list)\n",
    "        morph_list.append(doc_morph)\n",
    "\n",
    "\n",
    "# Srcsを埋めていく\n",
    "for i in range(len(Chunk_list)):\n",
    "    Chunk_list[Chunk_list[i].dst].srcs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "用いる\tを\t道具を\n",
      "する\tて を\t用いて 知能を\n",
      "指す\tを\t一分野を\n",
      "代わる\tに を\t人間に 知的行動を\n",
      "行う\tて に\t代わって コンピューターに\n",
      "する\tと も\t研究分野とも される\n"
     ]
    }
   ],
   "source": [
    "# morphの最左の動詞を抽出，なければNoneを返す\n",
    "def get_verb_in_sentence(chunk):\n",
    "    for morph in chunk.morph_list:\n",
    "        if morph.pos == \"動詞\":\n",
    "            return morph.base\n",
    "    return None\n",
    "\n",
    "# ppじは助詞(postpositional particle)のこと\n",
    "# 助詞のリストを返す\n",
    "def get_pp_in_sentence(chunk):\n",
    "    pp_list = []\n",
    "    for morph in chunk.morph_list:\n",
    "        if morph.pos == \"助詞\":\n",
    "            pp_list.append(morph.base)\n",
    "    return pp_list\n",
    "\n",
    "def get_sentence(chunk):\n",
    "    remove_list = [\"（\", \"）\", \"、\", \"。\", \"「\", \"」\", \"『\", \"』\", \"〈\", \"〉\"]\n",
    "    sentence = \"\"\n",
    "    for morph in chunk.morph_list:\n",
    "        if morph.surface not in remove_list:\n",
    "            sentence += morph.surface\n",
    "    return sentence\n",
    "\n",
    "\n",
    "for chunk in Chunk_list:\n",
    "    has_verb = get_verb_in_sentence(chunk)\n",
    "    if has_verb == None:    #動詞がなかったら飛ばす\n",
    "        continue\n",
    "    #動詞があれば，その文節に係っている(srcs)助詞を抽出する\n",
    "    srcs_pp_list = []\n",
    "    srcs_sentence_list = []\n",
    "    for src in chunk.srcs:\n",
    "        srcs_pp_list += get_pp_in_sentence(Chunk_list[src])\n",
    "        srcs_sentence_list.append(get_sentence(Chunk_list[src]))\n",
    "    # srcs_pp_listを基準にsrcs_sentence_listも並べ替える\n",
    "    pairs = zip(srcs_pp_list,srcs_sentence_list)\n",
    "    sorted_pairs = sorted(pairs)\n",
    "    sorted_srcs_pp_list, sorted_srcs_sentence_list = zip(*sorted_pairs)\n",
    "\n",
    "    srcs_text = \" \".join(sorted_srcs_pp_list)\n",
    "    srcs_sentence = \" \".join(sorted_srcs_sentence_list)\n",
    "    print(f\"{has_verb}\\t{srcs_text}\\t{srcs_sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**リーダブルコード**  \n",
    "- P16 具体的な名前を使う\n",
    "- P19 単語に情報を追加する\n",
    "- P51 コードを段落に分割する"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
