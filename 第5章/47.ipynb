{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "class Morph():\n",
    "    def __init__(self, doc_list):\n",
    "        self.surface = doc_list[0]\n",
    "        self.base = doc_list[7]\n",
    "        self.pos = doc_list[1]\n",
    "        self.pos1= doc_list[2]\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"{\" + f\"surface: {self.surface}, base: {self.base}, pos: {self.pos}, pos1: {self.pos1}\" + \"}\"\n",
    "\n",
    "class Chunk():\n",
    "    def __init__(self,morph_list,dst,srcs):\n",
    "        self.morph_list = morph_list\n",
    "        self.dst = int(dst)\n",
    "        self.srcs = srcs\n",
    "\n",
    "    def __str__(self):\n",
    "        morphs_str = ', '.join(str(morph) for morph in self.morph_list) # Morphオブジェクトのリストを文字列に変換\n",
    "        return f\"Morphs: [{morphs_str}], Dst: {self.dst}, Srcs: {self.srcs}\"\n",
    "\n",
    "# 文節番号と係り先番号の取得\n",
    "def find_feature_number(text):\n",
    "    pattern = r\"^\\* \\d+ ([^\\d]*\\d+)D\"\n",
    "    match = re.search(pattern, text)\n",
    "    if match:\n",
    "        dst_number = match.group(1)  # 文節番号  # 係先番号\n",
    "        return dst_number\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file=\"ai.ja.txt.parsed\", mode=\"r\", encoding=\"utf-8\")as f:\n",
    "    docs = f.readlines()\n",
    "\n",
    "Chunk_list = []\n",
    "chunk_flag = False\n",
    "# srcの追加処理は後で行う\n",
    "for doc in docs[5:]:\n",
    "    if doc == \"EOS\\n\":\n",
    "        chunk = Chunk(morph_list,dst_num,[])\n",
    "        Chunk_list.append(chunk)\n",
    "        break\n",
    "    # *から始まる行とEOSの行は飛ばす\n",
    "    if doc[0]==\"*\":\n",
    "        if chunk_flag:\n",
    "            chunk = Chunk(morph_list,dst_num,[])\n",
    "            Chunk_list.append(chunk)\n",
    "        dst_num = find_feature_number(doc)\n",
    "        morph_list = []\n",
    "        chunk_flag = True\n",
    "    else:\n",
    "        doc = doc.replace(\"\\t\",\",\").replace(\"\\n\", \"\")\n",
    "        doc_list = doc.split(\",\")\n",
    "        doc_morph = Morph(doc_list)\n",
    "        morph_list.append(doc_morph)\n",
    "\n",
    "\n",
    "# Srcsを埋めていく\n",
    "for i in range(len(Chunk_list)):\n",
    "    Chunk_list[Chunk_list[i].dst].srcs.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "行動を代わる\tに\t人間に\n"
     ]
    }
   ],
   "source": [
    "# morphの最左の動詞を抽出，なければNoneを返す\n",
    "def get_verb_in_sentence(chunk):\n",
    "    for morph in chunk.morph_list:\n",
    "        if morph.pos == \"動詞\":\n",
    "            return morph.base\n",
    "    return None\n",
    "\n",
    "# ppじは助詞(postpositional particle)のこと\n",
    "# 助詞のリストを返す\n",
    "def get_pp_in_sentence(chunk):\n",
    "    for morph in chunk.morph_list:\n",
    "        if morph.pos == \"助詞\":\n",
    "            return morph.base\n",
    "    return None\n",
    "\n",
    "def get_sentence(chunk):\n",
    "    remove_list = [\"（\", \"）\", \"、\", \"。\", \"「\", \"」\", \"『\", \"』\", \"〈\", \"〉\"]\n",
    "    sentence = \"\"\n",
    "    for morph in chunk.morph_list:\n",
    "        if morph.surface not in remove_list:\n",
    "            sentence += morph.surface\n",
    "    return sentence\n",
    "\n",
    "def get_sahen_josi_in_sentence(chunk):\n",
    "    for i in range(len(chunk.morph_list)-1):\n",
    "        if chunk.morph_list[i].pos1 == \"サ変接続\" and chunk.morph_list[i+1].pos == \"助詞\":\n",
    "            return chunk.morph_list[i].surface + chunk.morph_list[i+1].surface\n",
    "    return None\n",
    "\n",
    "\n",
    "for chunk in Chunk_list:\n",
    "    has_verb = get_verb_in_sentence(chunk)\n",
    "    if has_verb == None:    #動詞がなかったら飛ばす\n",
    "        continue\n",
    "    #動詞があれば，その文節に係っている(srcs)助詞を抽出する\n",
    "    srcs_pp_list = []\n",
    "    srcs_sentence_list = []\n",
    "    sahen_josi_flag = False #サ変接続+助詞が存在するかのフラッグ\n",
    "    # srcsの中の文節でサ変接続+助詞があるか確認，あれば抽出，なければNone\n",
    "    for src in chunk.srcs:\n",
    "        has_sahen_josi = get_sahen_josi_in_sentence(Chunk_list[src])\n",
    "        if has_sahen_josi != None:\n",
    "            #サ変+助詞を変数に格納\n",
    "            sahen_josi = has_sahen_josi\n",
    "            sahen_josi_flag = True\n",
    "\n",
    "    if sahen_josi_flag == False:\n",
    "        continue\n",
    "    for src in chunk.srcs:\n",
    "        has_sahen_josi = get_sahen_josi_in_sentence(Chunk_list[src])\n",
    "        if has_sahen_josi == None:\n",
    "            has_pp = get_pp_in_sentence(Chunk_list[src])\n",
    "            if has_pp != None:\n",
    "                srcs_pp_list.append(has_pp)\n",
    "                srcs_sentence_list.append(get_sentence(Chunk_list[src]))\n",
    "\n",
    "    if srcs_pp_list != []:\n",
    "        print(f\"{sahen_josi}{has_verb}\\t\",end = \"\")\n",
    "        pairs = zip(srcs_pp_list,srcs_sentence_list)\n",
    "        sorted_pairs = sorted(pairs)\n",
    "        sorted_srcs_pp_list, sorted_srcs_sentence_list = zip(*sorted_pairs)\n",
    "        for pp in sorted_srcs_pp_list:\n",
    "            print(pp, end=\"\")\n",
    "        print(\"\\t\", end=\"\")\n",
    "        for sentence in sorted_srcs_sentence_list:\n",
    "            print(sentence, end = \"\")\n",
    "        print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
